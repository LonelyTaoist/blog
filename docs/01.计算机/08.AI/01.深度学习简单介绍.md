---
title: 深度学习简单介绍
date: 2024/04/18 19:49:23
permalink: /computer/AI/1
categories:
  - 计算机
  - AI
  - 深度学习
tags:
  -
---

# 深度学习简单介绍

## 概述

**深度学习**是机器学习的分支，是一种以人工神经网络为架构，对资料进行表征学习的算法。深度学习中的形容词“深度”是指在网络中使用多层。早期的工作表明，线性感知器不能成为通用分类器，但具有非多项式激活函数和一个无限宽度隐藏层的网络可以成为通用分类器。

从数学角度理解深度网络[链接](http://staff.ustc.edu.cn/~lgliu/Resources/DL/What_is_DeepLearning.html)

### 神经元

输入数据特征 x，通过输出层神经元激活函数 σ（例如 sigmoid 函数）将输入的特征经由 sigmoid(wx + b) 的计算后再后输出。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/neuron.png)

### 深度神经网络

深度神经网络包含了三种网络层：输入层、隐藏层及输出层。

- 输入层：为数据特征输入层，输入数据特征个数就对应着网络的神经元数。
- 隐藏层：即网络的中间层，隐藏层层数可以为 0 或者很多层，其作用接受前一层网络输出作为当前的输入值，并计算输出当前结果到下一层。隐藏层是神经网络性能的关键，通常由含激活函数的神经元组成，以进一步加工出高层次抽象的特征，以增强网络的非线性表达。隐藏网络层数直接影响模型的拟合效果。
- 输出层：为最终结果输出的网络层。

数据特征 x 从输入层输入，每层的计算结果由前一层传递到下一层，最终到输出层输出计算结果。每个网络层由一定数量的神经元组成，神经元可以视为一个个的计算单元，对输入进行加权求和，故其计算结果由神经元包含的权重(即模型参数 w)直接控制，神经元内还可以包含激活函数，可以对加权求和的结果进一步做非线性的计算，如 sigmoid(wx + b)。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/DNN.png)


### 激活函数

如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，最终的输出都是输入的线性组合。 激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数。

- sigmoid

    ![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/sigmoid.png)

- tanh

    ![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/tanh.png)

- ReLU

    ![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/relu.png)

### 损失函数

- 均方差损失 (Mean Square Error，MSE)

$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(y_i- \hat{y_i} )^2
$$

​	均方误差损失又称为二次损失、L2损失，常用于回归预测任务中。均方误差函数通过计算预测值和实际值之间距离（即误差）的平方来衡量模型优劣。即预测值和真实值越接近，两者的均方差就越小。

- 交叉熵损失函数 (cross entropy)
  $$
  H(p,q) = - \sum_x p(x)\log{q(x)}
  $$
  

  在物理学中，“熵”被用来表示热力学系统所呈现的无序程度。香农将这一概念引入信息论领域，提出了“信息熵”概念，通过对数函数来测量信息的不确定性。交叉熵刻画了两个概率分布之间的距离，旨在描绘通过概率分布 q 来表达概率分布 p 的困难程度。根据公式不难理解，交叉熵越小，两个概率分布 p 和 q 越接近。

## 模型训练过程

数据划分：训练集 train，验证集 valid，测试集 test

1. 训练集数据输入模型正向传播。

2. 计算损失函数。

3. 反向传播更新模型参数，以最小化损失函数。

4. 验证集数据输入模型，并评估模型结果，选择多轮循环训练后的最优解。

5. 测试集进行模型测试

通过损失函数(Loss Function)来评价模型的拟合程度，当损失函数值下降，我们就认为模型在拟合的路上又前进了一步，最终模型对训练数据集拟合的最好的情况是在损失函数值最小的时候。

我们可以通过让参数在损失函数的“场”中，向着损失函数值减小的方向移动，最终在收敛的时候，得到一个极小值的近似解。为了让损失函数的数值下降，那么就需要使用优化算法进行优化，其中，损失函数值下降最快的方向称为负梯度方向。最终优化器（例如梯度下降法）在深度学习反向传播过程中，指引损失函数的各个参数往正确的方向更新合适的大小，使得更新后的各个参数让损失函数值不断逼近全局最小。

### [可视化网站](https://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)

### 模型调优

- 学习率

    学习率对应着优化器更新参数时的大小。但是选择一个好的学习率是非常困难的。太小的学习率导致收敛非常缓慢，而太大的学习率则会阻碍收敛，导致损失函数在最优点附近震荡甚至发散。甚至如果我们的数据比较稀疏，特征有非常多不同的频率，那么此时我们可能并不想要以相同的程度更新他们，而需要动态变化的针对不同参数的学习率。

    ![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/learning_rate.png)

- 正则化

    正则化可以避免算法过拟合，过拟合通常发生在算法学习的输入数据无法反应真实的分布且存在一些噪声的情况。过去数年，研究者提出和开发了多种适合机器学习算法的正则化方法，如数据增强、L2 正则化（权重衰减）、L1 正则化、Dropout、Drop Connect、随机池化和早停等。

    ![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/overfitting.png)

- batch_size

    更大的batch size会得到更精确的梯度估计值，但会降低收敛速度且增大资源开销。同时对于凸问题，局部最优即为全局最优的情况下，准确的梯度计算可以使梯度更新朝着正确的方向进行，以较快的速度达到全局最优解。但在真实的神经网络训练过程中，我们通常遇到的都是非凸问题，此时，局部最优解不是全局最优解，而传统的梯度下降法在这种情况下很容易陷入局部最优点或鞍点。

- 参数初始化

    当参数权重被初始化为相同值时，经过数学计算可以发现更新后的参数在每一层内都是相同的。同时，无论经过多少次网络训练，相同网络层内的参数值都是相同的，这会导致网络在学习时没有重点，对所有的特征处理相同，这很可能导致模型无法收敛训练失败，这种现象被称为对称失效。

## 时序模型简介

### RNN

**循环神经网络**（Recurrent Neural Network，RNN）是一个非常经典的面向序列的模型，可以对时序信号进行建模。进一步讲，它只有一个物理 RNN 单元，但是这个 RNN 单元可以按照时间步骤进行展开，在每个时间步骤接收当前时间步的输入和上一个时间步的输出，然后进行计算得出本时间步的输出。

**公式表达：**


$$
h_t=\sigma(Wx_t+Vh_{t-1}+b)
$$


![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/RNN.png)

**缺陷:**

在真实的任务训练过程中，RNN 存在一个明显的缺陷，那就是当处理很长的序列时，网络内部的信息会逐渐变得越来越复杂（可能导致梯度消失和梯度爆炸），以至于超过网络的记忆能力，使得最终的输出信息变得混乱无用。



### LSTM

**长短时记忆网络**（Long Short Term Memory，LSTM）是循环神经网络的一种，它为了解决 RNN 自身的缺陷，向RNN单元中引入了**门机制**进行改善。同 RNN 一样，LSTM 也只有一个物理 LSTM 单元，并按照时间步骤展开处理时序数据。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/LSTM.png)



每个时刻 LSTM 会接收 3 种数据：当前时刻输入 $x_t$，上一个时刻的状态向量 $c_{t-1}$，上一个时刻的隐状态向量 $h_{t-1}$ 。

同时输出 2 种数据：当前时刻的状态向量 $c_t$，当前时刻的隐状态向量 $h_t$ 。

LSTM单元内部组件：

- **输入门 $i_t$**：控制当前时刻的输入信息需要向状态向量 $c_t$ 中注入哪些信息。
- **遗忘门 $f_t$**：控制前一时刻的状态向量 $c_{t-1}$ 需要被屏蔽/遗忘哪些信息。
- **输出门 $o_t$**：控制当前时刻的状态向量 $c_t$ 需要对外输出哪些信息，最终输出的信息即为 $h_t$。

**公式表达：**
$$
i_t = \sigma(W_fx_t + U_ih_{t-1} + b_i) 
$$

$$
f_t = \sigma(W_fx_t + U_fh_{t-1} + b_f)
$$

$$
o_t = \sigma(W_ox_t + U_oh_{t-1} + b_o)
$$

$$
a_t = tanh(W_ax_t + U_ah_{t-1} + b_a)
$$

$$
c_t = f_t \cdot c_{t-1} + i_t \cdot a_t
$$

$$
h_t = o_t \cdot tanh(c_t)
$$

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/LSTM2.png)

显然，LSTM 单元状态 $c_t$ 的更新是对上一个时刻的状态 $c_{t-1}$ 进行有选择的遗忘，对当前时刻的待输入信息 $a_t$ 将有选择的输入，最后将两者的结果进行相加，表示向当前 LSTM 单元即融入了以前的状态信息 $c_{t-1}$，同时又注入了当前最新的信息 $a_t$。在计算出当前时刻的状态向量 $c_t$ 后，就可以根据该状态向量对外进行输出了。

**缺陷:**

增加了三个不同的门，参数较多，训练起来比较困难。

### GRU

**门控循环单元**（Gated Recurrent Units）是对于LSTM结构复杂性的优化。其只含有两个门控结构，且在超参数全部调优的情况下，二者性能相当，但是GRU结构更为简单，训练样本较少，易实现。

GRU 在 LSTM 的基础上主要做出了两点改变 ：

- GRU 只有两个门。GRU 将 LSTM 中的输入门和遗忘门合二为一，称为更新门（update gate），下图中的 $z_t$，控制前面记忆信息能够继续保留到当前时刻的数据量，或者说决定有多少前一时间步的信息和当前时间步的信息要被继续传递到未来；GRU 的另一个门称为重置门（reset gate），下图中的 $r_t$ ，控制要遗忘多少过去的信息。

- GRU 取消了进行线性自更新的记忆单元（memory cell），而是直接在隐藏单元中利用门控直接进行线性自更新。GRU的逻辑图如下图所示。

**公式表达：**
$$
z_t=\sigma(W_z \cdot [h_{t-1},x_t])
$$

$$
r_t=\sigma(W_r \cdot [h_{t-1},x_t])
$$

$$
\tilde{h}=tanh(W \cdot [r_t \odot h_{t-1},x_t])
$$

$$
h_t=(1-z_t) \odot h_{t-1} + z_t \odot \tilde{h_{t}}
$$

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/gru.png)





### Transfomer

Transformer 主要由 encoder 和 decoder 两部分组成。在 Transformer 的论文中，encoder 和 decoder 均由 6 个 encoder layer 和 decoder layer 组成，通常我们称之为 encoder block。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/the_transformer_encoder_decoder_stack.png)

Transfomer 网络架构中，编码器和解码器没有采用 RNN 或 CNN 等网络架构，而是采用完全依赖于注意力机制的架构。其改进了 RNN 被人诟病的训练慢的特点，利用 self-attention 实现快速并行。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/transformer.png)

#### Self Attention

首先，自注意力机制（self-attention）会计算出三个新的向量，分别称为Query、Key、Value，这三个向量是用输入向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的并在反向传播的过程中会一直进行更新。

接着计算 self-attention 的分数值，该分数值决定了序列中不同部分之间的关注程度。这个分数值的计算方法是 Query 与 Key 做点乘。然后把得到的结果做一个 softmax 的计算，得到的结果即是每个序列位置对于当前位置的相关性大小，当然当前位置自身的相关性肯定会很大。

最后把 Value 和 softmax 得到的权值进行相乘并相加，得到的结果即是 self-attetion 在当前节点的值。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/self_attention.png)

**Multi-head Attention**

把上述 QKV 矩阵初始化多组并在最后生成多个 self-attention 值，即使多头注意力机制。另外，也可以针对序列内每个向量的不同部分分别做 self-attention 最后在拼接起来。

![](https://blog-web-image.oss-cn-shanghai.aliyuncs.com/computer/AI/1/transformer_self_attention.png)
